# Dutch-GPT2 Project Overview

This repository is dedicated to a side project exploring Large Language Models (LLMs) with a focus on the Dutch language.

## Dataset Utilization

We leverage the "OSCAR-2109" dataset version from the [OSCAR corpus](https://oscar-corpus.com/), particularly targeting the Dutch language subset identified as `deduplicated_nl`. The OSCAR corpus offers an extensive collection of data invaluable for various natural language processing and linguistic research tasks.

<div align="center">

[![Tutorial: Let's build GPT](https://huggingface.co/blog/assets/01_how-to-train/oscar.png)](https://oscar-corpus.com/)

</div>

## Methodological Approach

My approach is inspired by the training methods outlined in the [Hugging Face Tutorial](https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/01_how_to_train.ipynb#scrollTo=LTXXutqeDzPi). However there are some deprecated techniques in the original guide,  have refined and improved upon these strategies. 

[![Tutorial: Let's build GPT](https://huggingface.co/blog/assets/01_how-to-train/EsperBERTo-thumbnail-v2.png)](https://huggingface.co/blog/how-to-train)